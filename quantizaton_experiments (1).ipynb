{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c670508-1bc7-4be2-9146-e35e432a783a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1274/1751918087.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1274/1751918087.py:15: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 01:08:43.566751: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-19 01:08:43.566885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-19 01:08:43.674916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 01:08:43.909459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-19 01:08:45.744370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display, HTML\n",
    "from difflib import ndiff\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face –∏ Unsloth\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TextStreamer, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# –ú–æ–∏ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2dcadf-4fcb-4779-8206-46ff4e5987b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgalkin-vova1\u001b[0m (\u001b[33mgalkin-vova42\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "HF_ACCESS_TOKEN = \"\"\n",
    "WANDB_PROJECT_NAME = \"quantization_experiments\"\n",
    "WANDB_NOTEBOOK_NAME = 'text-correction-experiments' # –ò–º—è —ç—Ç–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞ –¥–ª—è WandB\n",
    "\n",
    "# –õ–æ–≥–∏–Ω –≤ WandB\n",
    "try:\n",
    "    WANDB_API_KEY = ''\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    anonymous = None\n",
    "except:\n",
    "    anonymous = \"must\" # –∏–ª–∏ \"allow\" –∏–ª–∏ \"never\"\n",
    "    wandb.login(anonymous=anonymous, relogin=True)\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048 # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # –ú–æ–¥—É–ª–∏ –¥–ª—è LoRA\n",
    "\n",
    "# –ò–º–µ–Ω–∞ –º–æ–¥–µ–ª–µ–π\n",
    "# –î–ª—è –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 1: –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å. –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ç–æ—á–Ω–æ–µ –∏–º—è 8B –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è.\n",
    "BASE_MODEL_NAME_FULL_PRECISION = \"unsloth/Qwen3-0.6B\"\n",
    "# –î–ª—è –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 3: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Unsloth\n",
    "BASE_MODEL_NAME_UNSLOTH_QUANTIZED = \"unsloth/Qwen3-0.6B-unsloth-bnb-4bit\"\n",
    "\n",
    "# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "OUTPUT_DIR_LORA_FULL = \"./results_lora_full_precision\"\n",
    "OUTPUT_DIR_PTQ_8BIT = \"./results_ptq_8bit\"\n",
    "OUTPUT_DIR_PTQ_4BIT = \"./results_ptq_4bit\"\n",
    "OUTPUT_DIR_QLORA = \"./results_qlora\"\n",
    "MERGED_MODEL_DIR_FOR_PTQ = \"./merged_lora_full_model_for_ptq\"\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –¥–ª—è —Å–±–æ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "all_experiment_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40797da2-3a56-4cb7-9030-790d4888811d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: 555\n",
      "–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: 139\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å—ã—Ä–æ–π):\n",
      "{'text_with_errors': '–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–∏–ø–ª–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. –°—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ –Ω–µ—É–∫–ª–æ–Ω–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–∞—è–Ω–∏—é –ª–µ–¥–Ω–∏–∫–æ–≤ –ø–æ–≤—ã—à–µ–Ω–∏—é —É—Ä–æ–≤–Ω—è –º–∏—Ä–æ–≤–æ–≥–æ –æ–∫–µ–∞–Ω–∞ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—é –∫–ª–∏–º–∞—Ç–∞. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤ –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –°–∂–∏–≥–∞–Ω–∏–µ –∏—Å–∫–æ–ø–∞–µ–º–æ–≥–æ —Ç–æ–ø–ª–∏–≤–∞ –≤—ã—Ä—É–±–∫–∞ –ª–µ—Å–æ–≤ –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —É–≥–ª–µ–∫–∏—Å–ª–æ–≥–æ –≥–∞–∑–∞ –≤ –≤–æ–∑–¥—É—Ö–µ. –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–º–∏: –∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ –ø—Ä–∏–±—Ä–µ–∂–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π —É—Å–∏–ª–µ–Ω–∏–µ –∑–∞—Å—É—Ö –∏ –Ω–∞–≤–æ–¥–Ω–µ–Ω–∏–π –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –≤–∏–¥–æ–≤ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏ —Ä–∞—Å—Ç–µ–Ω–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —É—Å–∏–ª–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–µ—Å–æ–≤. –ö–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ –±–æ—Ä—å–±—É —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Å–æ–∫—Ä–∞—â–∞—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –∏ –≤—ã–±–∏—Ä–∞—è —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —á–∏—Å—Ç—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã.', 'corrected_text': '–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. –°—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ –Ω–µ—É–∫–ª–æ–Ω–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–∞—è–Ω–∏—é –ª–µ–¥–Ω–∏–∫–æ–≤, –ø–æ–≤—ã—à–µ–Ω–∏—é —É—Ä–æ–≤–Ω—è –º–∏—Ä–æ–≤–æ–≥–æ –æ–∫–µ–∞–Ω–∞ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—é –∫–ª–∏–º–∞—Ç–∞. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤ –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –°–∂–∏–≥–∞–Ω–∏–µ –∏—Å–∫–æ–ø–∞–µ–º–æ–≥–æ —Ç–æ–ø–ª–∏–≤–∞, –≤—ã—Ä—É–±–∫–∞ –ª–µ—Å–æ–≤ –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —É–≥–ª–µ–∫–∏—Å–ª–æ–≥–æ –≥–∞–∑–∞ –≤ –≤–æ–∑–¥—É—Ö–µ. –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–º–∏: –∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ –ø—Ä–∏–±—Ä–µ–∂–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π, —É—Å–∏–ª–µ–Ω–∏–µ –∑–∞—Å—É—Ö –∏ –Ω–∞–≤–æ–¥–Ω–µ–Ω–∏–π, –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –≤–∏–¥–æ–≤ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏ —Ä–∞—Å—Ç–µ–Ω–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —É—Å–∏–ª–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤, —Ä–∞–∑–≤–∏—Ç–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–µ—Å–æ–≤. –ö–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ –±–æ—Ä—å–±—É —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–æ–∫—Ä–∞—â–∞—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –∏ –≤—ã–±–∏—Ä–∞—è —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —á–∏—Å—Ç—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã.'}\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ChatML):\n",
      "[{'content': '–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç. –û—á–µ–Ω—å –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –≤—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –í—ã –º–æ–∂–µ—Ç–µ –≤–Ω–æ—Å–∏—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∏ –Ω–µ—á–∞—Å—Ç—ã–µ –ø—Ä–∞–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç —Ç–æ–ª—å–∫–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –ø—É–Ω–∫—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏. –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å–ª–æ–≤–Ω–æ. –ù–µ –æ–±—ä—è—Å–Ω—è–π—Ç–µ —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.', 'role': 'system'}, {'content': '–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–∏–ø–ª–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. –°—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ –Ω–µ—É–∫–ª–æ–Ω–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–∞—è–Ω–∏—é –ª–µ–¥–Ω–∏–∫–æ–≤ –ø–æ–≤—ã—à–µ–Ω–∏—é —É—Ä–æ–≤–Ω—è –º–∏—Ä–æ–≤–æ–≥–æ –æ–∫–µ–∞–Ω–∞ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—é –∫–ª–∏–º–∞—Ç–∞. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤ –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –°–∂–∏–≥–∞–Ω–∏–µ –∏—Å–∫–æ–ø–∞–µ–º–æ–≥–æ —Ç–æ–ø–ª–∏–≤–∞ –≤—ã—Ä—É–±–∫–∞ –ª–µ—Å–æ–≤ –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —É–≥–ª–µ–∫–∏—Å–ª–æ–≥–æ –≥–∞–∑–∞ –≤ –≤–æ–∑–¥—É—Ö–µ. –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–º–∏: –∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ –ø—Ä–∏–±—Ä–µ–∂–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π —É—Å–∏–ª–µ–Ω–∏–µ –∑–∞—Å—É—Ö –∏ –Ω–∞–≤–æ–¥–Ω–µ–Ω–∏–π –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –≤–∏–¥–æ–≤ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏ —Ä–∞—Å—Ç–µ–Ω–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —É—Å–∏–ª–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–µ—Å–æ–≤. –ö–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ –±–æ—Ä—å–±—É —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Å–æ–∫—Ä–∞—â–∞—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –∏ –≤—ã–±–∏—Ä–∞—è —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —á–∏—Å—Ç—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã.', 'role': 'user'}, {'content': '–ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏. –°—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ –Ω–µ—É–∫–ª–æ–Ω–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–∞—è–Ω–∏—é –ª–µ–¥–Ω–∏–∫–æ–≤, –ø–æ–≤—ã—à–µ–Ω–∏—é —É—Ä–æ–≤–Ω—è –º–∏—Ä–æ–≤–æ–≥–æ –æ–∫–µ–∞–Ω–∞ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—é –∫–ª–∏–º–∞—Ç–∞. –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–∏—á–∏–Ω–æ–π —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤ –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –°–∂–∏–≥–∞–Ω–∏–µ –∏—Å–∫–æ–ø–∞–µ–º–æ–≥–æ —Ç–æ–ø–ª–∏–≤–∞, –≤—ã—Ä—É–±–∫–∞ –ª–µ—Å–æ–≤ –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é —É–≥–ª–µ–∫–∏—Å–ª–æ–≥–æ –≥–∞–∑–∞ –≤ –≤–æ–∑–¥—É—Ö–µ. –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–º–∏: –∑–∞—Ç–æ–ø–ª–µ–Ω–∏–µ –ø—Ä–∏–±—Ä–µ–∂–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π, —É—Å–∏–ª–µ–Ω–∏–µ –∑–∞—Å—É—Ö –∏ –Ω–∞–≤–æ–¥–Ω–µ–Ω–∏–π, –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –≤–∏–¥–æ–≤ –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏ —Ä–∞—Å—Ç–µ–Ω–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —É—Å–∏–ª–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–∞—Ä–Ω–∏–∫–æ–≤—ã—Ö –≥–∞–∑–æ–≤, —Ä–∞–∑–≤–∏—Ç–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–µ—Å–æ–≤. –ö–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ –±–æ—Ä—å–±—É —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —ç–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–æ–∫—Ä–∞—â–∞—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –∏ –≤—ã–±–∏—Ä–∞—è —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —á–∏—Å—Ç—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "non_reasoning_dataset = load_dataset(\"galkinv42/text-correction-ru\", token=HF_ACCESS_TOKEN)\n",
    "dataset_split = non_reasoning_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "dataset_train_raw = dataset_split[\"train\"]\n",
    "dataset_test_raw = dataset_split[\"test\"]\n",
    "\n",
    "# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "SYSTEM_PROMPT = '–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç. –û—á–µ–Ω—å –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –≤—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –í—ã –º–æ–∂–µ—Ç–µ –≤–Ω–æ—Å–∏—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∏ –Ω–µ—á–∞—Å—Ç—ã–µ –ø—Ä–∞–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç —Ç–æ–ª—å–∫–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –ø—É–Ω–∫—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏. –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å–ª–æ–≤–Ω–æ. –ù–µ –æ–±—ä—è—Å–Ω—è–π—Ç–µ —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.'\n",
    "\n",
    "def format_chatml(example):\n",
    "    return {\n",
    "        \"conv\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"text_with_errors\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"corrected_text\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset_train_formatted = dataset_train_raw.map(format_chatml)\n",
    "dataset_test_formatted = dataset_test_raw.map(format_chatml)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –ø–æ–∑–∂–µ, –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º,\n",
    "# —Ç–∞–∫ –∫–∞–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "def formatting_prompts_func(examples, tokenizer_to_use):\n",
    "    convos = examples[\"conv\"]\n",
    "    texts = [tokenizer_to_use.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(dataset_train_raw)}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(dataset_test_raw)}\")\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å—ã—Ä–æ–π):\")\n",
    "print(dataset_train_raw[0])\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ChatML):\")\n",
    "print(dataset_train_formatted[0][\"conv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2c0d5-b21a-47ee-b422-5e7cde3f7865",
   "metadata": {},
   "source": [
    "### –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: –û–±—É—á–µ–Ω–∏–µ LoRa –Ω–∞ \"–ø–æ–ª–Ω–æ–π\" –º–æ–¥–µ–ª–∏ Qwen (–±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏)\n",
    "\n",
    "–ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qwen/Qwen2-7B-Instruct –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞. Unsloth –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏ –Ω–µ–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ –Ω–∏–º LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8a1943-8ef2-43db-a9bf-779bd60b64ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment_1_full_lora():\n",
    "    print(\"\\n--- –ó–∞–ø—É—Å–∫ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 1: LoRa –Ω–∞ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ ---\")\n",
    "    \n",
    "    run_name_wandb = \"exp1_lora_full_precision\"\n",
    "    wandb.init(entity='galkin-vova42', project=WANDB_PROJECT_NAME, name=run_name_wandb, job_type=\"train\", config={\"experiment_type\": \"lora_full_precision\", \"base_model\": BASE_MODEL_NAME_FULL_PRECISION}, reinit=False)\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ë–ï–ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
    "    # Unsloth —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –ø–æ–ª–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_NAME_FULL_PRECISION,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=None, # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∏–ª–∏ torch.float16, torch.bfloat16\n",
    "        load_in_4bit=False, # –í–ê–ñ–ù–û: –±–µ–∑ 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
    "        load_in_8bit=False, # –í–ê–ñ–ù–û: –±–µ–∑ 8-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
    "        token=HF_ACCESS_TOKEN,\n",
    "    )\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ç–µ–∫—É—â–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º\n",
    "    dataset_train = Dataset.from_dict(formatting_prompts_func(dataset_train_formatted, tokenizer))\n",
    "    dataset_test = Dataset.from_dict(formatting_prompts_func(dataset_test_formatted, tokenizer))\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=32,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_alpha=32, # –û–±—ã—á–Ω–æ alpha = r –∏–ª–∏ 2*r\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    \n",
    "    trainer_config = SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        output_dir=OUTPUT_DIR_LORA_FULL,\n",
    "        per_device_train_batch_size=3,\n",
    "        per_device_eval_batch_size=3,\n",
    "        gradient_accumulation_steps=4, # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º GA\n",
    "        warmup_steps=10, # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        do_eval=True,\n",
    "        eval_steps=20, # –ß–∞—â–µ –æ—Ü–µ–Ω–∫–∞\n",
    "        save_steps=20,\n",
    "        save_total_limit=2, # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é\n",
    "        greater_is_better=False,\n",
    "        load_best_model_at_end=True,\n",
    "        eval_strategy='steps',\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        max_steps=100,\n",
    "        learning_rate=1e-4, # –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞ (1e-4 to 2e-5)\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\", # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –Ω–µ–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        report_to=\"wandb\",\n",
    "        fp16=not torch.cuda.is_bf16_supported(), # –ò—Å–ø–æ–ª—å–∑—É–µ–º fp16 –µ—Å–ª–∏ bf16 –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset_train,\n",
    "        eval_dataset=dataset_test,\n",
    "        args=trainer_config,\n",
    "    )\n",
    "\n",
    "    # –ü–∞–º—è—Ç—å –¥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    gpu_stats = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3) if torch.cuda.is_available() else 0\n",
    "    \n",
    "    print(\"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è LoRa –Ω–∞ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    # –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        print(f\"–ü–∏–∫–æ–≤–∞—è VRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ–ª–Ω–∞—è LoRa): {used_memory_for_lora:.3f} GB\")\n",
    "        wandb.log({\"peak_vram_train_gb\": used_memory_for_lora})\n",
    "\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–∞–¥–∞–ø—Ç–µ—Ä–æ–≤)\n",
    "    # SFTTrainer —Å load_best_model_at_end=True —É–∂–µ –∑–∞–≥—Ä—É–∑–∏–ª –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "    best_model_path = os.path.join(OUTPUT_DIR_LORA_FULL, \"best_lora_adapters\")\n",
    "    model.save_pretrained(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)\n",
    "    print(f\"–õ—É—á—à–∏–µ LoRa –∞–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {best_model_path}\")\n",
    "    wandb.save(os.path.join(best_model_path, \"*\")) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ wandb\n",
    "\n",
    "    print(\"–û—Ü–µ–Ω–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)...\")\n",
    "    \n",
    "    predictions = []\n",
    "    references_batch_eval = [] # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ —Å –≥–ª–æ–±–∞–ª—å–Ω–æ–π references\n",
    "    sample_inference_latencies = [] # –ë—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º—è –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Å—Ä–µ–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏\n",
    "    num_samples_for_eval = min(100, len(dataset_test_raw)) # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏\n",
    "    eval_batch_size = 8\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "    eval_data = []\n",
    "    for i in range(num_samples_for_eval):\n",
    "        example = dataset_test_raw[i]\n",
    "        eval_data.append({\n",
    "            \"user_content\": example[\"text_with_errors\"],\n",
    "            \"reference_text\": example[\"corrected_text\"]\n",
    "        })\n",
    "\n",
    "    # –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º (–º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats() # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∑–∞–º–µ—Ä–∞ –ø–∏–∫–æ–≤–æ–π –ø–∞–º—è—Ç–∏ –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
    "        initial_inference_vram = torch.cuda.max_memory_allocated()\n",
    "    else:\n",
    "        initial_inference_vram = 0\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    for batch_data in tqdm(create_batches(eval_data, eval_batch_size)):\n",
    "        batch_prompts_text = []\n",
    "        current_references = []\n",
    "\n",
    "        for item in batch_data:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": item[\"user_content\"]}\n",
    "            ]\n",
    "            prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            batch_prompts_text.append(prompt_text)\n",
    "            current_references.append(item[\"reference_text\"].strip())\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_prompts_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length if tokenizer.model_max_length < MAX_SEQ_LENGTH else MAX_SEQ_LENGTH # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ–µ –∏–∑ –¥–≤—É—Ö\n",
    "        ).to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=min(512, MAX_SEQ_LENGTH),\n",
    "                temperature=0.01,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        sample_inference_latencies.append(end_time - start_time) # –í—Ä–µ–º—è –Ω–∞ –≤–µ—Å—å –±–∞—Ç—á\n",
    "        \n",
    "        input_ids_lengths = inputs.attention_mask.sum(dim=1).tolist() # –î–ª–∏–Ω—ã —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ (–±–µ–∑ –ø–∞–¥–¥–∏–Ω–≥–∞)\n",
    "\n",
    "        batch_output_texts = []\n",
    "        for i in range(outputs.shape[0]):\n",
    "            \n",
    "            generated_tokens_only = outputs[i, input_ids_lengths[i]:]\n",
    "            output_text = tokenizer.decode(generated_tokens_only, skip_special_tokens=True)\n",
    "            batch_output_texts.append(output_text.strip())\n",
    "            \n",
    "        predictions.extend(batch_output_texts)\n",
    "        references_batch_eval.extend(current_references)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        peak_inference_vram_bytes = torch.cuda.max_memory_allocated() \n",
    "        peak_inference_vram = peak_inference_vram_bytes / (1024**3)\n",
    "    else:\n",
    "        peak_inference_vram = 0\n",
    "    \n",
    "    bleu_score = calculate_bleu(predictions, references_batch_eval)\n",
    "    cer_score = calculate_cer(predictions, references_batch_eval)\n",
    "    \n",
    "    total_inference_time = sum(sample_inference_latencies)\n",
    "    avg_latency_per_sample = total_inference_time / num_samples_for_eval if num_samples_for_eval > 0 else -1\n",
    "\n",
    "\n",
    "    print(f\"BLEU ({model_name_str if 'model_name_str' in locals() else 'Current Model'}): {bleu_score:.4f}\")\n",
    "    print(f\"CER ({model_name_str if 'model_name_str' in locals() else 'Current Model'}): {cer_score:.4f}\")\n",
    "    print(f\"Avg Inference Latency per sample ({model_name_str if 'model_name_str' in locals() else 'Current Model'}): {avg_latency_per_sample:.4f} s\")\n",
    "    print(f\"Peak VRAM during batched inference ({model_name_str if 'model_name_str' in locals() else 'Current Model'}): {peak_inference_vram:.3f} GB\")\n",
    "\n",
    "\n",
    "    trainable_params, total_params = get_model_trainable_parameters(model)\n",
    "    \n",
    "    results = {\n",
    "        \"experiment_group\": \"LoRa Full Precision\",\n",
    "        \"model_name\": f\"LoRa ({BASE_MODEL_NAME_FULL_PRECISION})\",\n",
    "        \"val_loss\": trainer.state.best_model_checkpoint_loss if hasattr(trainer.state, \"best_model_checkpoint_loss\") else trainer_stats.training_loss,\n",
    "        \"bleu\": bleu_score,\n",
    "        \"cer\": cer_score,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"total_params\": total_params,\n",
    "        \"vram_train_peak_gb\": used_memory_for_lora if torch.cuda.is_available() else 0,\n",
    "        \"vram_inference_peak_gb\": peak_inference_vram,\n",
    "        \"avg_inference_latency_s\": avg_latency_per_sample,\n",
    "        \"model_path\": best_model_path,\n",
    "    }\n",
    "    all_experiment_results.append(results)\n",
    "    wandb.log({\n",
    "        \"eval_bleu\": bleu_score, \"eval_cer\": cer_score, \"avg_inference_latency\": avg_latency_per_sample,\n",
    "        \"trainable_params\": trainable_params, \"total_params\": total_params,\n",
    "        \"peak_inference_vram_gb\": peak_inference_vram\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞\n",
    "    clear_gpu_memory(model, trainer)\n",
    "    return best_model_path, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b28742-96e7-4667-94b3-fed9c42a4777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- –ó–∞–ø—É—Å–∫ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 1: LoRa –Ω–∞ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/text-correction/all_exps/wandb/run-20250518_205835-zwa8ac00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/zwa8ac00' target=\"_blank\">exp1_lora_full_precision</a></strong> to <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/zwa8ac00' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/zwa8ac00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.6: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 555/555 [00:01<00:00, 342.08 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:01<00:00, 134.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è LoRa –Ω–∞ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 555 | Num Epochs = 3 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 4 x 1) = 12\n",
      " \"-____-\"     Trainable parameters = 20,185,088/616,235,008 (3.28% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:25, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.277200</td>\n",
       "      <td>1.236925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.075700</td>\n",
       "      <td>1.168622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.123300</td>\n",
       "      <td>1.149048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.248100</td>\n",
       "      <td>1.142154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.087500</td>\n",
       "      <td>1.139461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–∏–∫–æ–≤–∞—è VRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ–ª–Ω–∞—è LoRa): 2.008 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 9 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ—É—á—à–∏–µ LoRa –∞–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: ./results_lora_full_precision/best_lora_adapters\n",
      "–û—Ü–µ–Ω–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:438: UserWarning: An output with one or more elements was resized since it had shape [1, 8, 1024], which does not match the required output shape [8, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "1it [00:21, 21.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2it [00:41, 20.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "3it [01:02, 20.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "4it [01:23, 20.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "5it [01:44, 20.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "6it [02:04, 20.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "7it [02:25, 20.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "8it [02:46, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "9it [03:07, 20.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "10it [03:28, 20.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "11it [03:48, 20.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "12it [04:09, 20.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:438: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 1024], which does not match the required output shape [4, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "13it [04:29, 20.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (Current Model): 0.1648\n",
      "CER (Current Model): 0.8239\n",
      "Avg Inference Latency per sample (Current Model): 2.6946 s\n",
      "Peak VRAM during batched inference (Current Model): 3.524 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inference_latency</td><td>‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>eval_bleu</td><td>‚ñÅ</td></tr><tr><td>eval_cer</td><td>‚ñÅ</td></tr><tr><td>peak_inference_vram_gb</td><td>‚ñÅ</td></tr><tr><td>peak_vram_train_gb</td><td>‚ñÅ</td></tr><tr><td>total_params</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÇ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ</td></tr><tr><td>trainable_params</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inference_latency</td><td>2.69462</td></tr><tr><td>eval/loss</td><td>1.13946</td></tr><tr><td>eval/runtime</td><td>4.6706</td></tr><tr><td>eval/samples_per_second</td><td>29.761</td></tr><tr><td>eval/steps_per_second</td><td>10.063</td></tr><tr><td>eval_bleu</td><td>0.16478</td></tr><tr><td>eval_cer</td><td>0.82386</td></tr><tr><td>peak_inference_vram_gb</td><td>3.5242</td></tr><tr><td>peak_vram_train_gb</td><td>2.008</td></tr><tr><td>total_flos</td><td>4089505608105984.0</td></tr><tr><td>total_params</td><td>616235008</td></tr><tr><td>train/epoch</td><td>2.12973</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>0.27834</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0875</td></tr><tr><td>train_loss</td><td>1.21314</td></tr><tr><td>train_runtime</td><td>147.8446</td></tr><tr><td>train_samples_per_second</td><td>8.117</td></tr><tr><td>train_steps_per_second</td><td>0.676</td></tr><tr><td>trainable_params</td><td>20185088</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp1_lora_full_precision</strong> at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/zwa8ac00' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/zwa8ac00</a><br> View project at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 9 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_205835-zwa8ac00/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n",
      "–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1 –∑–∞–≤–µ—Ä—à–µ–Ω. –ê–¥–∞–ø—Ç–µ—Ä—ã –≤: ./results_lora_full_precision/best_lora_adapters\n"
     ]
    }
   ],
   "source": [
    "path_to_lora_full_adapters, tokenizer_lora_full = run_experiment_1_full_lora()\n",
    "print(f\"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1 –∑–∞–≤–µ—Ä—à–µ–Ω. –ê–¥–∞–ø—Ç–µ—Ä—ã –≤: {path_to_lora_full_adapters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6059080-391c-43c8-9b27-b0d80b6ad5d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### QLora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c62936-2548-4289-b188-75e71d194c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment_3_qlora(results_accumulator_list):\n",
    "    print(\"\\n--- –ó–∞–ø—É—Å–∫ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 3: QLoRa ---\")\n",
    "    \n",
    "    ranks = [16, 32] \n",
    "    alphas_map = {16: [32], 32: [64]}\n",
    "\n",
    "    best_params_qlora = {\"r\": 0, \"alpha\": 0, \"loss\": float('inf')}\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è HPO\n",
    "    base_q_model_for_hpo, tokenizer_qlora = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_NAME_UNSLOTH_QUANTIZED,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=True,\n",
    "        token=HF_ACCESS_TOKEN,\n",
    "    )\n",
    "    \n",
    "    dataset_train_q = Dataset.from_dict(formatting_prompts_func(dataset_train_formatted, tokenizer_qlora))\n",
    "    dataset_test_q = Dataset.from_dict(formatting_prompts_func(dataset_test_formatted, tokenizer_qlora))\n",
    "\n",
    "    print(\"–ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ r –∏ alpha –¥–ª—è QLoRa...\")\n",
    "    for r_val in ranks:\n",
    "        for alpha_val in alphas_map[r_val]:\n",
    "            current_run_name_hpo = f\"qlora_hpo_r{r_val}_a{alpha_val}\"\n",
    "            print(f\"\\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ QLoRa —Å r={r_val}, alpha={alpha_val}\")\n",
    "            \n",
    "            # –î–ª—è –∫–∞–∂–¥–æ–π HPO –∏—Ç–µ—Ä–∞—Ü–∏–∏ - –Ω–æ–≤—ã–π run\n",
    "            wandb.init(entity='galkin-vova42', project=WANDB_PROJECT_NAME, name=current_run_name_hpo, job_type=\"hpo_qlora\", \n",
    "                       config={\"experiment_type\": \"qlora_hpo\", \"r\": r_val, \"alpha\": alpha_val, \"base_model\": BASE_MODEL_NAME_UNSLOTH_QUANTIZED},\n",
    "                       reinit=True)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º PEFT –º–æ–¥–µ–ª—å. Unsloth –¥–æ–ª–∂–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞–¥–∞–ø—Ç–µ—Ä–æ–≤.\n",
    "            # –î–ª—è –ø–æ–ª–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞—Ç—å base_q_model_for_hpo, –Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º —Ç–∞–∫.\n",
    "            # –ï—Å–ª–∏ –±—É–¥—É—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å \"–Ω–∞—Å–ª–æ–µ–Ω–∏–µ–º\" –∞–¥–∞–ø—Ç–µ—Ä–æ–≤, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ü–∏–∫–ª–µ.\n",
    "            # Unsloth FastLanguageModel.get_peft_model –¥–æ–ª–∂–µ–Ω —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã.\n",
    "            \n",
    "            # –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ base_q_model_for_hpo, –µ—Å–ª–∏ get_peft_model —Ä–∞–±–æ—Ç–∞–µ—Ç inplace (—Ö–æ—Ç—è –Ω–µ –¥–æ–ª–∂–µ–Ω)\n",
    "            # –∏–ª–∏ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ —É–¥–∞–ª—è—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –∑–∞–≥—Ä—É–∂–∞—Ç—å temp_base_model,\n",
    "            # –Ω–æ —ç—Ç–æ –∑–∞–º–µ–¥–ª–∏—Ç HPO. –ü–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ Unsloth.\n",
    "            if 'peft_q_model_hpo' in locals(): # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–π HPO –º–æ–¥–µ–ª–∏\n",
    "                 clear_gpu_memory(peft_q_model_hpo)\n",
    "\n",
    "            peft_q_model_hpo = FastLanguageModel.get_peft_model(\n",
    "                base_q_model_for_hpo, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –∏ —Ç—É –∂–µ –±–∞–∑–æ–≤—É—é –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "                r=r_val,\n",
    "                target_modules=TARGET_MODULES,\n",
    "                lora_alpha=alpha_val,\n",
    "                lora_dropout=0,\n",
    "                bias=\"none\",\n",
    "                use_gradient_checkpointing=\"unsloth\", # \"unsloth\" –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ True\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            trainer_q_hpo = SFTTrainer(\n",
    "                model=peft_q_model_hpo,\n",
    "                tokenizer=tokenizer_qlora,\n",
    "                train_dataset=dataset_train_q,\n",
    "                eval_dataset=dataset_test_q, \n",
    "                args=SFTConfig(\n",
    "                    dataset_text_field=\"text\",\n",
    "                    output_dir=f\"./results_qlora_hpo_r{r_val}_a{alpha_val}\", # –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è\n",
    "                    per_device_train_batch_size=2, # –ö–∞–∫ –≤ –≤–∞—à–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –¥–ª—è QLoRA\n",
    "                    gradient_accumulation_steps=4,\n",
    "                    warmup_steps=5,\n",
    "                    max_steps=30, \n",
    "                    learning_rate=2e-4,\n",
    "                    logging_steps=5,\n",
    "                    optim=\"adamw_8bit\", \n",
    "                    weight_decay=0.01,\n",
    "                    lr_scheduler_type=\"linear\",\n",
    "                    seed=42,\n",
    "                    report_to=\"wandb\",\n",
    "                    do_eval=True,\n",
    "                    eval_strategy=\"steps\",\n",
    "                    eval_steps=10,\n",
    "                    save_strategy=\"no\", \n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            trainer_q_hpo.train()\n",
    "            current_eval_loss = float('inf')\n",
    "            for log_entry in reversed(trainer_q_hpo.state.log_history):\n",
    "                if 'eval_loss' in log_entry:\n",
    "                    current_eval_loss = log_entry['eval_loss']\n",
    "                    break\n",
    "            \n",
    "            print(f\"HPO QLoRa r={r_val}, alpha={alpha_val} -> eval_loss: {current_eval_loss}\")\n",
    "            wandb.log({\"hpo_eval_loss\": current_eval_loss, \"r\": r_val, \"alpha\": alpha_val})\n",
    "\n",
    "            if current_eval_loss < best_params_qlora[\"loss\"]:\n",
    "                best_params_qlora[\"loss\"] = current_eval_loss\n",
    "                best_params_qlora[\"r\"] = r_val\n",
    "                best_params_qlora[\"alpha\"] = alpha_val\n",
    "            \n",
    "            wandb.finish()\n",
    "            # –û—á–∏—â–∞–µ–º —Ç–æ–ª—å–∫–æ peft_q_model_hpo –∏ trainer, –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Å—Ç–∞—é—Ç—Å—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ HPO\n",
    "            clear_gpu_memory(peft_q_model_hpo, trainer_q_hpo) \n",
    "\n",
    "    # –û—á–∏—â–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è HPO\n",
    "    clear_gpu_memory(base_q_model_for_hpo) # tokenizer_qlora –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "    print(f\"\\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è QLoRa: r={best_params_qlora['r']}, alpha={best_params_qlora['alpha']} (loss: {best_params_qlora['loss']:.4f})\")\n",
    "\n",
    "    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ QLoRa —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "    model_name_final_qlora = f\"QLoRa r={best_params_qlora['r']},a={best_params_qlora['alpha']} ({BASE_MODEL_NAME_UNSLOTH_QUANTIZED})\"\n",
    "    run_name_wandb_final_qlora = \"exp3_qlora_final_training\"\n",
    "    wandb.init(entity='galkin-vova42', project=WANDB_PROJECT_NAME, name=run_name_wandb_final_qlora, job_type=\"train_qlora_final\", \n",
    "               config={\"experiment_type\": \"qlora_final\", \n",
    "                       \"r\": best_params_qlora['r'], \n",
    "                       \"alpha\": best_params_qlora['alpha'], \n",
    "                       \"base_model\": BASE_MODEL_NAME_UNSLOTH_QUANTIZED,\n",
    "                       \"best_hpo_loss\": best_params_qlora['loss']},\n",
    "               reinit=True) # –ù–æ–≤—ã–π run –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    final_base_q_model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_NAME_UNSLOTH_QUANTIZED, max_seq_length=MAX_SEQ_LENGTH, load_in_4bit=True, token=HF_ACCESS_TOKEN\n",
    "    )\n",
    "\n",
    "    final_peft_q_model = FastLanguageModel.get_peft_model(\n",
    "        final_base_q_model,\n",
    "        r=best_params_qlora['r'],\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_alpha=best_params_qlora['alpha'],\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    trainer_q_final = SFTTrainer(\n",
    "        model=final_peft_q_model,\n",
    "        tokenizer=tokenizer_qlora, # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "        train_dataset=dataset_train_q,\n",
    "        eval_dataset=dataset_test_q,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            output_dir=OUTPUT_DIR_QLORA, \n",
    "            per_device_train_batch_size=2, # –ö–∞–∫ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ\n",
    "            per_device_eval_batch_size=2,  # –î–æ–±–∞–≤–∏–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=10, \n",
    "            max_steps=100,  # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 200-500)\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=42,\n",
    "            report_to=\"wandb\",\n",
    "            do_eval=True,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=20, \n",
    "            save_strategy=\"steps\", \n",
    "            save_steps=20,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True, \n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            fp16=not torch.cuda.is_bf16_supported(), # –î–ª—è QLoRA —á–∞—Å—Ç–æ bf16=True –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    start_gpu_memory_qlora = round(torch.cuda.max_memory_reserved() / 1024**3, 3) if torch.cuda.is_available() else 0\n",
    "    trainer_stats_qlora = trainer_q_final.train()\n",
    "    \n",
    "    peak_vram_train_qlora_gb = 0\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory_qlora_train = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
    "        peak_vram_train_qlora_gb = round(used_memory_qlora_train - start_gpu_memory_qlora, 3)\n",
    "        print(f\"–ü–∏–∫–æ–≤–∞—è VRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (QLoRa final): {peak_vram_train_qlora_gb:.3f} GB\")\n",
    "        wandb.log({\"peak_vram_train_gb\": peak_vram_train_qlora_gb})\n",
    "\n",
    "    best_qlora_model_path = os.path.join(OUTPUT_DIR_QLORA, \"best_qlora_adapters\")\n",
    "    final_peft_q_model.save_pretrained(best_qlora_model_path) \n",
    "    tokenizer_qlora.save_pretrained(best_qlora_model_path)\n",
    "    print(f\"–õ—É—á—à–∏–µ QLoRa –∞–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {best_qlora_model_path}\")\n",
    "    if os.path.exists(best_qlora_model_path):\n",
    "        wandb.save(os.path.join(best_qlora_model_path, \"*\"))\n",
    "\n",
    "\n",
    "    # --- –ù–∞—á–∞–ª–æ –±–ª–æ–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π QLoRa –º–æ–¥–µ–ª–∏ ---\n",
    "    print(f\"–û—Ü–µ–Ω–∫–∞ –ª—É—á—à–µ–π QLoRa –º–æ–¥–µ–ª–∏: {model_name_final_qlora} (–ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)...\")\n",
    "    eval_batch_size_qlora = 8 # –ö–∞–∫ –≤ –≤–∞—à–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–º Exp1\n",
    "\n",
    "    predictions_q_final = []\n",
    "    references_q_final = []\n",
    "    latencies_q_final = []\n",
    "    num_samples_for_eval = min(100, len(dataset_test_raw))\n",
    "\n",
    "    eval_data_q_final = []\n",
    "    for i in range(num_samples_for_eval):\n",
    "        example = dataset_test_raw[i] \n",
    "        eval_data_q_final.append({\n",
    "            \"user_content\": example[\"text_with_errors\"],\n",
    "            \"reference_text\": example[\"corrected_text\"]\n",
    "        })\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        _ = torch.cuda.max_memory_allocated()\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # final_peft_q_model —É–∂–µ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ device\n",
    "\n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å pad_token\n",
    "    if tokenizer_qlora.pad_token is None:\n",
    "        tokenizer_qlora.pad_token = tokenizer_qlora.eos_token\n",
    "        # –ú–æ–¥–µ–ª—å —É–∂–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–∞ Unsloth –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å load_in_4bit\n",
    "        # if hasattr(final_peft_q_model.config, 'pad_token_id') and final_peft_q_model.config.pad_token_id is None:\n",
    "        # final_peft_q_model.config.pad_token_id = tokenizer_qlora.eos_token_id\n",
    "    \n",
    "    for batch_data in tqdm(create_batches(eval_data_q_final, eval_batch_size_qlora), desc=f\"–û—Ü–µ–Ω–∫–∞ {model_name_final_qlora}\"):\n",
    "        batch_prompts_text = []\n",
    "        current_references = []\n",
    "\n",
    "        for item in batch_data:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": item[\"user_content\"]}\n",
    "            ]\n",
    "            prompt_text = tokenizer_qlora.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            batch_prompts_text.append(prompt_text)\n",
    "            current_references.append(item[\"reference_text\"].strip())\n",
    "        \n",
    "        current_max_len_qlora = MAX_SEQ_LENGTH\n",
    "        if hasattr(tokenizer_qlora, 'model_max_length') and tokenizer_qlora.model_max_length:\n",
    "            current_max_len_qlora = min(tokenizer_qlora.model_max_length, MAX_SEQ_LENGTH)\n",
    "\n",
    "        inputs = tokenizer_qlora(\n",
    "            batch_prompts_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=current_max_len_qlora\n",
    "        ).to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = final_peft_q_model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=min(512, MAX_SEQ_LENGTH),\n",
    "                temperature=0.01, # –ö–∞–∫ –≤ –≤–∞—à–µ–º Exp1\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer_qlora.pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        latencies_q_final.append(end_time - start_time)\n",
    "        \n",
    "        input_ids_lengths = inputs.attention_mask.sum(dim=1).tolist()\n",
    "\n",
    "        batch_output_texts = []\n",
    "        for i in range(outputs.shape[0]):\n",
    "            generated_tokens_only = outputs[i, input_ids_lengths[i]:]\n",
    "            output_text = tokenizer_qlora.decode(generated_tokens_only, skip_special_tokens=True)\n",
    "            batch_output_texts.append(output_text.strip())\n",
    "            \n",
    "        predictions_q_final.extend(batch_output_texts)\n",
    "        references_q_final.extend(current_references)\n",
    "    \n",
    "    peak_inference_vram_qlora_gb = 0\n",
    "    if torch.cuda.is_available():\n",
    "        peak_inference_vram_qlora_bytes = torch.cuda.max_memory_allocated() \n",
    "        peak_inference_vram_qlora_gb = peak_inference_vram_qlora_bytes / (1024**3)\n",
    "\n",
    "    bleu_score_q = calculate_bleu(predictions_q_final, references_q_final)\n",
    "    cer_score_q = calculate_cer(predictions_q_final, references_q_final)\n",
    "    \n",
    "    total_inference_time_q = sum(latencies_q_final)\n",
    "    avg_latency_per_sample_q = total_inference_time_q / num_samples_for_eval if num_samples_for_eval > 0 else -1\n",
    "\n",
    "    print(f\"BLEU ({model_name_final_qlora}): {bleu_score_q:.4f}\")\n",
    "    print(f\"CER ({model_name_final_qlora}): {cer_score_q:.4f}\")\n",
    "    print(f\"Avg Inference Latency per sample ({model_name_final_qlora}): {avg_latency_per_sample_q:.4f} s\")\n",
    "    print(f\"Peak VRAM during batched inference ({model_name_final_qlora}): {peak_inference_vram_qlora_gb:.3f} GB\")\n",
    "    # --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ ---\n",
    "\n",
    "    trainable_params_q, total_params_q = get_model_trainable_parameters(final_peft_q_model)\n",
    "    \n",
    "    results_q = {\n",
    "        \"experiment_group\": \"QLoRa\",\n",
    "        \"model_name\": model_name_final_qlora,\n",
    "        \"val_loss\": trainer_q_final.state.best_model_checkpoint_loss if hasattr(trainer_q_final.state, 'best_model_checkpoint_loss') and trainer_q_final.state.best_model_checkpoint_loss is not None else -1,\n",
    "        \"bleu\": bleu_score_q,\n",
    "        \"cer\": cer_score_q,\n",
    "        \"trainable_params\": trainable_params_q,\n",
    "        \"total_params\": total_params_q, \n",
    "        \"vram_train_peak_gb\": peak_vram_train_qlora_gb,\n",
    "        \"vram_inference_peak_gb\": peak_inference_vram_qlora_gb,\n",
    "        \"avg_inference_latency_s\": avg_latency_per_sample_q,\n",
    "        \"model_path\": best_qlora_model_path,\n",
    "        \"best_hpo_r\": best_params_qlora['r'],\n",
    "        \"best_hpo_alpha\": best_params_qlora['alpha'],\n",
    "    }\n",
    "    results_accumulator_list.append(results_q)\n",
    "    wandb.log({\n",
    "        \"eval_bleu\": bleu_score_q, \"eval_cer\": cer_score_q, \"avg_inference_latency_per_sample\": avg_latency_per_sample_q,\n",
    "        \"trainable_params\": trainable_params_q, \"total_params_base_quant\": total_params_q, \n",
    "        \"peak_inference_vram_gb\": peak_inference_vram_qlora_gb,\n",
    "        \"best_val_loss_final_train\": results_q[\"val_loss\"]\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n",
    "    clear_gpu_memory(final_peft_q_model, trainer_q_final, final_base_q_model, tokenizer_qlora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc402ae5-74a3-4204-8e76-a4cb2126649e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- –ó–∞–ø—É—Å–∫ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 3: QLoRa ---\n",
      "==((====))==  Unsloth 2025.5.6: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ r –∏ alpha –¥–ª—è QLoRa...\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ QLoRa —Å r=16, alpha=32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/text-correction/all_exps/wandb/run-20250518_224833-mp9vs3g2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/mp9vs3g2' target=\"_blank\">qlora_hpo_r16_a32</a></strong> to <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/mp9vs3g2' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/mp9vs3g2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 555/555 [00:01<00:00, 340.13 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:01<00:00, 135.60 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 555 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 10,092,544/6,000,000,000 (0.17% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.410700</td>\n",
       "      <td>1.356854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>1.240758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.305500</td>\n",
       "      <td>1.226838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO QLoRa r=16, alpha=32 -> eval_loss: 1.2268377542495728\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>hpo_eval_loss</td><td>‚ñÅ</td></tr><tr><td>r</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>32</td></tr><tr><td>eval/loss</td><td>1.22684</td></tr><tr><td>eval/runtime</td><td>4.3905</td></tr><tr><td>eval/samples_per_second</td><td>31.659</td></tr><tr><td>eval/steps_per_second</td><td>7.972</td></tr><tr><td>hpo_eval_loss</td><td>1.22684</td></tr><tr><td>r</td><td>16</td></tr><tr><td>total_flos</td><td>735778897920000.0</td></tr><tr><td>train/epoch</td><td>0.43165</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>0.46103</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.3055</td></tr><tr><td>train_loss</td><td>1.3785</td></tr><tr><td>train_runtime</td><td>55.9846</td></tr><tr><td>train_samples_per_second</td><td>4.287</td></tr><tr><td>train_steps_per_second</td><td>0.536</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">qlora_hpo_r16_a32</strong> at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/mp9vs3g2' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/mp9vs3g2</a><br> View project at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_224833-mp9vs3g2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ QLoRa —Å r=32, alpha=64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/text-correction/all_exps/wandb/run-20250518_224943-8mqgczaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/8mqgczaa' target=\"_blank\">qlora_hpo_r32_a64</a></strong> to <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/8mqgczaa' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/8mqgczaa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 555/555 [00:01<00:00, 339.73 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:01<00:00, 126.89 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 555 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 20,185,088/6,000,000,000 (0.34% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.327800</td>\n",
       "      <td>1.277131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.260300</td>\n",
       "      <td>1.217028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.279100</td>\n",
       "      <td>1.202683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO QLoRa r=32, alpha=64 -> eval_loss: 1.202683448791504\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÜ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÉ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÉ‚ñà</td></tr><tr><td>hpo_eval_loss</td><td>‚ñÅ</td></tr><tr><td>r</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>64</td></tr><tr><td>eval/loss</td><td>1.20268</td></tr><tr><td>eval/runtime</td><td>4.3897</td></tr><tr><td>eval/samples_per_second</td><td>31.665</td></tr><tr><td>eval/steps_per_second</td><td>7.973</td></tr><tr><td>hpo_eval_loss</td><td>1.20268</td></tr><tr><td>r</td><td>32</td></tr><tr><td>total_flos</td><td>752260345233408.0</td></tr><tr><td>train/epoch</td><td>0.43165</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>0.53962</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.2791</td></tr><tr><td>train_loss</td><td>1.33371</td></tr><tr><td>train_runtime</td><td>53.4955</td></tr><tr><td>train_samples_per_second</td><td>4.486</td></tr><tr><td>train_steps_per_second</td><td>0.561</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">qlora_hpo_r32_a64</strong> at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/8mqgczaa' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/8mqgczaa</a><br> View project at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_224943-8mqgczaa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared.\n",
      "Could not move an object to CPU: `.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\n",
      "GPU memory cleared.\n",
      "\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è QLoRa: r=32, alpha=64 (loss: 1.2027)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/text-correction/all_exps/wandb/run-20250518_225051-6xp8bmnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/6xp8bmnb' target=\"_blank\">exp3_qlora_final_training</a></strong> to <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/6xp8bmnb' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/6xp8bmnb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.6: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 555/555 [00:01<00:00, 321.66 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:01<00:00, 124.19 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 555 | Num Epochs = 2 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 20,185,088/6,000,000,000 (0.34% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:52, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.231800</td>\n",
       "      <td>1.210760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.210600</td>\n",
       "      <td>1.160366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.055900</td>\n",
       "      <td>1.140321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.021600</td>\n",
       "      <td>1.132806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.075900</td>\n",
       "      <td>1.130949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–∏–∫–æ–≤–∞—è VRAM –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (QLoRa final): 0.000 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 9 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ—É—á—à–∏–µ QLoRa –∞–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: ./results_qlora/best_qlora_adapters\n",
      "–û—Ü–µ–Ω–∫–∞ –ª—É—á—à–µ–π QLoRa –º–æ–¥–µ–ª–∏: QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit) (–ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 0it [00:00, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:438: UserWarning: An output with one or more elements was resized since it had shape [1, 8, 1024], which does not match the required output shape [8, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:443: UserWarning: An output with one or more elements was resized since it had shape [1, 8, 1024], which does not match the required output shape [8, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W, out = out)\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 1it [00:28, 28.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 2it [00:55, 27.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 3it [01:23, 27.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 4it [01:51, 27.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 5it [02:18, 27.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 6it [02:46, 27.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 7it [03:14, 27.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 8it [03:42, 27.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 9it [04:09, 27.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 10it [04:37, 27.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 11it [05:05, 27.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 12it [05:32, 27.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:438: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 1024], which does not match the required output shape [4, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:443: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 1024], which does not match the required output shape [4, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W, out = out)\n",
      "–û—Ü–µ–Ω–∫–∞ QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit): 13it [05:59, 27.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit)): 0.1633\n",
      "CER (QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit)): 0.9105\n",
      "Avg Inference Latency per sample (QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit)): 3.5902 s\n",
      "Peak VRAM during batched inference (QLoRa r=32,a=64 (unsloth/Qwen3-0.6B-unsloth-bnb-4bit)): 2.898 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_inference_latency_per_sample</td><td>‚ñÅ</td></tr><tr><td>best_val_loss_final_train</td><td>‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÑ‚ñÅ‚ñÇ‚ñà‚ñÉ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñà‚ñá‚ñÅ‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñà‚ñá‚ñÅ‚ñÜ</td></tr><tr><td>eval_bleu</td><td>‚ñÅ</td></tr><tr><td>eval_cer</td><td>‚ñÅ</td></tr><tr><td>peak_inference_vram_gb</td><td>‚ñÅ</td></tr><tr><td>peak_vram_train_gb</td><td>‚ñÅ</td></tr><tr><td>total_params_base_quant</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñá‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>trainable_params</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_inference_latency_per_sample</td><td>3.59024</td></tr><tr><td>best_val_loss_final_train</td><td>-1</td></tr><tr><td>eval/loss</td><td>1.13095</td></tr><tr><td>eval/runtime</td><td>6.9526</td></tr><tr><td>eval/samples_per_second</td><td>19.993</td></tr><tr><td>eval/steps_per_second</td><td>10.068</td></tr><tr><td>eval_bleu</td><td>0.16333</td></tr><tr><td>eval_cer</td><td>0.91051</td></tr><tr><td>peak_inference_vram_gb</td><td>2.89801</td></tr><tr><td>peak_vram_train_gb</td><td>0</td></tr><tr><td>total_flos</td><td>2513150759927808.0</td></tr><tr><td>total_params_base_quant</td><td>408616960</td></tr><tr><td>train/epoch</td><td>1.43165</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>0.46899</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0759</td></tr><tr><td>train_loss</td><td>1.17667</td></tr><tr><td>train_runtime</td><td>174.3063</td></tr><tr><td>train_samples_per_second</td><td>4.59</td></tr><tr><td>train_steps_per_second</td><td>0.574</td></tr><tr><td>trainable_params</td><td>20185088</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp3_qlora_final_training</strong> at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments/runs/6xp8bmnb' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments/runs/6xp8bmnb</a><br> View project at: <a href='https://wandb.ai/galkin-vova42/quantization_experiments' target=\"_blank\">https://wandb.ai/galkin-vova42/quantization_experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 9 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_225051-6xp8bmnb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not move an object to CPU: `.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.\n",
      "GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "run_experiment_3_qlora(all_experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8f99e-8e4c-49a2-8618-b50cc7091a67",
   "metadata": {},
   "source": [
    "### –≠–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç 3. Full SFT + PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879f1f6e-4bf8-421b-a38e-0c79292907c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.6: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 555/555 [00:01<00:00, 284.08 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:01<00:00, 116.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_sft, tokenizer_sft = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_NAME_FULL_PRECISION,\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = False,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    ")\n",
    "\n",
    "for param in model_sft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "dataset_train_sft = Dataset.from_dict(formatting_prompts_func(dataset_train_formatted, tokenizer_sft))\n",
    "dataset_test_sft = Dataset.from_dict(formatting_prompts_func(dataset_test_formatted, tokenizer_sft))\n",
    "\n",
    "OUTPUT_DIR_FULL_SFT = 'text-correction/all_exps/results_full_sft'\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    output_dir=OUTPUT_DIR_FULL_SFT,\n",
    "    per_device_train_batch_size=3, # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á –∏–∑-–∑–∞ VRAM\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=16, # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º GA –¥–ª—è –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –º–∞–ª–æ–≥–æ –±–∞—Ç—á–∞\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,\n",
    "    learning_rate=5e-6, # –û–±—ã—á–Ω–æ –Ω–∏–∂–µ –¥–ª—è Full SFT (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2e-5, 1e-5, 5e-6)\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    # num_train_epochs=2,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    report_to=\"wandb\",\n",
    "    fp16=False, # bf16 –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ bf16\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    # gradient_checkpointing=True, # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –∑–¥–µ—Å—å –∏–ª–∏ Unsloth —É–∂–µ –º–æ–≥ –≤–∫–ª—é—á–∏—Ç—å\n",
    "    # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π VRAM, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ DeepSpeed:\n",
    "    # deepspeed=\"path/to/deepspeed_config.json\", # –µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥ DeepSpeed (Stage 2 –∏–ª–∏ 3)\n",
    ")\n",
    "\n",
    "trainer_sft = SFTTrainer(\n",
    "    model=model_sft,\n",
    "    tokenizer=tokenizer_sft,\n",
    "    train_dataset=dataset_train_sft,\n",
    "    eval_dataset=dataset_test_sft,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a622fc1-83c0-4f1a-bd14-9f193ccdce8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 555 | Num Epochs = 10 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 16 x 1) = 48\n",
      " \"-____-\"     Trainable parameters = 596,049,920/596,049,920 (100.00% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:50, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.623700</td>\n",
       "      <td>1.649266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.516800</td>\n",
       "      <td>1.494032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.375800</td>\n",
       "      <td>1.389468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.350300</td>\n",
       "      <td>1.312293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.287000</td>\n",
       "      <td>1.268899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.247700</td>\n",
       "      <td>1.245877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.223900</td>\n",
       "      <td>1.234134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.225700</td>\n",
       "      <td>1.229404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.177200</td>\n",
       "      <td>1.228148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.215100</td>\n",
       "      <td>1.227621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.3467343986034392, metrics={'train_runtime': 474.3145, 'train_samples_per_second': 10.12, 'train_steps_per_second': 0.211, 'total_flos': 1.533264131063808e+16, 'train_loss': 1.3467343986034392})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_sft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc629a13-ac4b-4df1-a110-9a29c3526ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û—Ü–µ–Ω–∫–∞ sft model: 0it [00:00, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:438: UserWarning: An output with one or more elements was resized since it had shape [1, 8, 1024], which does not match the required output shape [8, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 1it [00:15, 15.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 2it [00:30, 15.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 3it [00:45, 15.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 4it [01:01, 15.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 5it [01:16, 15.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 6it [01:31, 15.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 7it [01:47, 15.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 8it [02:02, 15.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 9it [02:17, 15.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 10it [02:33, 15.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 11it [02:48, 15.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 12it [03:03, 15.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/opt/conda/lib/python3.10/site-packages/unsloth/kernels/utils.py:438: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 1024], which does not match the required output shape [4, 1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "–û—Ü–µ–Ω–∫–∞ sft model: 13it [03:17, 15.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 0.2139\n",
      "CER 0.7020\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size_qlora = 8\n",
    "\n",
    "predictions_q_final = []\n",
    "references_q_final = []\n",
    "latencies_q_final = []\n",
    "num_samples_for_eval = min(100, len(dataset_test_raw))\n",
    "\n",
    "eval_data_q_final = []\n",
    "for i in range(num_samples_for_eval):\n",
    "    example = dataset_test_raw[i] \n",
    "    eval_data_q_final.append({\n",
    "        \"user_content\": example[\"text_with_errors\"],\n",
    "        \"reference_text\": example[\"corrected_text\"]\n",
    "    })\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    _ = torch.cuda.max_memory_allocated()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if tokenizer_sft.pad_token is None:\n",
    "    tokenizer_sft.pad_token = tokenizer_sft.eos_token\n",
    "\n",
    "for batch_data in tqdm(create_batches(eval_data_q_final, eval_batch_size_qlora), desc=f\"–û—Ü–µ–Ω–∫–∞ sft model\"):\n",
    "    batch_prompts_text = []\n",
    "    current_references = []\n",
    "\n",
    "    for item in batch_data:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": item[\"user_content\"]}\n",
    "        ]\n",
    "        prompt_text = tokenizer_sft.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        batch_prompts_text.append(prompt_text)\n",
    "        current_references.append(item[\"reference_text\"].strip())\n",
    "\n",
    "    current_max_len_qlora = MAX_SEQ_LENGTH\n",
    "    if hasattr(tokenizer_sft, 'model_max_length') and tokenizer_sft.model_max_length:\n",
    "        current_max_len_qlora = min(tokenizer_sft.model_max_length, MAX_SEQ_LENGTH)\n",
    "\n",
    "    inputs = tokenizer_sft(\n",
    "        batch_prompts_text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=current_max_len_qlora\n",
    "    ).to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_sft.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=min(512, MAX_SEQ_LENGTH),\n",
    "            temperature=0.01, # –ö–∞–∫ –≤ –≤–∞—à–µ–º Exp1\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer_sft.pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    latencies_q_final.append(end_time - start_time)\n",
    "\n",
    "    input_ids_lengths = inputs.attention_mask.sum(dim=1).tolist()\n",
    "\n",
    "    batch_output_texts = []\n",
    "    for i in range(outputs.shape[0]):\n",
    "        generated_tokens_only = outputs[i, input_ids_lengths[i]:]\n",
    "        output_text = tokenizer_sft.decode(generated_tokens_only, skip_special_tokens=True)\n",
    "        batch_output_texts.append(output_text.strip())\n",
    "\n",
    "    predictions_q_final.extend(batch_output_texts)\n",
    "    references_q_final.extend(current_references)\n",
    "\n",
    "peak_inference_vram_qlora_gb = 0\n",
    "if torch.cuda.is_available():\n",
    "    peak_inference_vram_qlora_bytes = torch.cuda.max_memory_allocated() \n",
    "    peak_inference_vram_qlora_gb = peak_inference_vram_qlora_bytes / (1024**3)\n",
    "\n",
    "bleu_score_q = calculate_bleu(predictions_q_final, references_q_final)\n",
    "cer_score_q = calculate_cer(predictions_q_final, references_q_final)\n",
    "\n",
    "total_inference_time_q = sum(latencies_q_final)\n",
    "avg_latency_per_sample_q = total_inference_time_q / num_samples_for_eval if num_samples_for_eval > 0 else -1\n",
    "\n",
    "print(f\"BLEU {bleu_score_q:.4f}\")\n",
    "print(f\"CER {cer_score_q:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "520f65ee-731a-4e65-bbe0-99beaacb7aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPTQConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "937f0279-5423-4d07-9030-a07dc0417b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3 isn't supported yet.\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "pretrained_model_dir = \"results_full_sft/checkpoint-100\"\n",
    "quantized_model_dir = \"quantized\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3053649b-d546-4739-975c-a1694d0674a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "616e0af4-5e40-4e71-9bb3-1038d7c143eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç. –û—á–µ–Ω—å –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –≤—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –í—ã –º–æ–∂–µ—Ç–µ –≤–Ω–æ—Å–∏—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∏ –Ω–µ—á–∞—Å—Ç—ã–µ –ø—Ä–∞–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç —Ç–æ–ª—å–∫–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –ø—É–Ω–∫—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏. –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å–ª–æ–≤–Ω–æ. –ù–µ –æ–±—ä—è—Å–Ω—è–π—Ç–µ —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.\n",
      "user\n",
      "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥–µ—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—à–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ, –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á, –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã, –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä, –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤, –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å—ë–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞, –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ, –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1), –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞ –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ.\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ, –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤ –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞ –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ, –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞, –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ. \n",
      "#1\n",
      "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ, –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤ –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞ –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0.\n"
     ]
    }
   ],
   "source": [
    "batch = dataset_test_sft['text'][2:3]\n",
    "\n",
    "inputs = tokenizer_sft(\n",
    "    batch, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=current_max_len_qlora\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_sft.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=min(512, MAX_SEQ_LENGTH),\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        pad_token_id=tokenizer_sft.pad_token_id,\n",
    "        eos_token_id=tokenizer_sft.eos_token_id  # Often helpful to add\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer_sft.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ee60f21-80e4-48c6-aee8-cbc82f43829b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1274/1980763572.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π <span style=\"color: red; text-decoration: line-through;\">–¥–∏—Ä–µ—Ç–æ—Ä</span> <span style=\"color: green;\">–¥–∏—Ä–µ–∫—Ç–æ—Ä</span> —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª <span style=\"color: red; text-decoration: line-through;\">–ê–ª–µ–∫—Å–∞–Ω–¥–µ—Ä</span> <span style=\"color: green;\">–ê–ª–µ–∫—Å–∞–Ω–¥—Ä</span> –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª <span style=\"color: red; text-decoration: line-through;\">(–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ</span> <span style=\"color: green;\">(–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ,</span> –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ <span style=\"color: red; text-decoration: line-through;\">–ø–æ–º–æ—à–Ω–∏–∫–æ–≤</span> <span style=\"color: green;\">–ø–æ–º–æ—â–Ω–∏–∫–æ–≤</span> –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª <span style=\"color: red; text-decoration: line-through;\">—Ä–µ—à–µ–Ω–∏–µ,</span> <span style=\"color: green;\">—Ä–µ—à–µ–Ω–∏–µ</span> –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ <span style=\"color: red; text-decoration: line-through;\">–º–∞—Ç—á,</span> <span style=\"color: green;\">–º–∞—Ç—á</span> –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ <span style=\"color: red; text-decoration: line-through;\">–∏–≥—Ä—ã,</span> <span style=\"color: green;\">–∏–≥—Ä—ã</span> –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ <span style=\"color: red; text-decoration: line-through;\">–∞—Ä–±–∏—Ç—Ä,</span> <span style=\"color: green;\">–∞—Ä–±–∏—Ç—Ä</span> –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä <span style=\"color: red; text-decoration: line-through;\">–ö—É—Ü–µ–≤,</span> <span style=\"color: green;\">–ö—É—Ü–µ–≤</span> –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç <span style=\"color: red; text-decoration: line-through;\">—Å–µ—Ä—å—ë–∑–Ω—ã–µ</span> <span style=\"color: green;\">—Å–µ—Ä—å–µ–∑–Ω—ã–µ</span> —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ <span style=\"color: red; text-decoration: line-through;\">–ì–∞–π–¥–∞—à–∞,</span> <span style=\"color: green;\">–ì–∞–π–¥–∞—à–∞</span> –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± <span style=\"color: red; text-decoration: line-through;\">–æ—Ç–º–µ–Ω–µ,</span> <span style=\"color: green;\">–æ—Ç–º–µ–Ω–µ</span> –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª <span style=\"color: red; text-decoration: line-through;\">(2:1),</span> <span style=\"color: green;\">(2:1)</span> –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ <span style=\"color: red; text-decoration: line-through;\">–∫–æ—Ä–∏–¥–æ—Ä–µ</span> <span style=\"color: green;\">–∫–æ—Ä–∏–¥–æ—Ä–µ,</span> –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ <span style=\"color: red; text-decoration: line-through;\">–ì–∞–¥–∂–∏–µ–≤–∞</span> <span style=\"color: green;\">–ì–∞–¥–∂–∏–µ–≤–∞,</span> –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from difflib import ndiff\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def format_diffs(original, corrected):\n",
    "    diffs = list(ndiff(original.split(), corrected.split()))\n",
    "    formatted_text = \"\"\n",
    "    for diff in diffs:\n",
    "        if diff.startswith('-'):\n",
    "            formatted_text += f'<span style=\"color: red; text-decoration: line-through;\">{diff[2:]}</span> '\n",
    "        elif diff.startswith('+'):\n",
    "            formatted_text += f'<span style=\"color: green;\">{diff[2:]}</span> '\n",
    "        elif diff.startswith(' '):\n",
    "            formatted_text += diff[2:] + ' '\n",
    "    return formatted_text\n",
    "\n",
    "input_text = '–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥–µ—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—à–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ, –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á, –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã, –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä, –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤, –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å—ë–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞, –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ, –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1), –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞ –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ.'\n",
    "output_text = '–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ, –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤ –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞ –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ, –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞, –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ.'\n",
    "\n",
    "display(HTML(format_diffs(input_text, output_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21b29a88-86d6-48ae-9159-434b6b3e7f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.6: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.526 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name='results_qlora/checkpoint-100',\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=None, # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∏–ª–∏ torch.float16, torch.bfloat16\n",
    "        load_in_4bit=False,\n",
    "        load_in_8bit=False,\n",
    "        token=HF_ACCESS_TOKEN,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9089f600-47e3-4c8b-9d2d-b2c182a8dfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç. –û—á–µ–Ω—å –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã –≤—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –í—ã –º–æ–∂–µ—Ç–µ –≤–Ω–æ—Å–∏—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∏ –Ω–µ—á–∞—Å—Ç—ã–µ –ø—Ä–∞–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç —Ç–æ–ª—å–∫–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ, –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –ø—É–Ω–∫—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏. –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å–ª–æ–≤–Ω–æ. –ù–µ –æ–±—ä—è—Å–Ω—è–π—Ç–µ —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.\n",
      "user\n",
      "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥–µ—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—à–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ, –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á, –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã, –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä, –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤, –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å—ë–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞, –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ, –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1), –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞ –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ.\n",
      "assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ, –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤ –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞ –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ, –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞, –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ. \n",
      "#–¢—É—Ä–Ω–∏—Ä #–§—É—Ç–±–æ–ª #–°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å #–ì–≤–∞—Ä–¥–µ–π—Ü—ã #–ì–∞–π–¥–∞—à\n"
     ]
    }
   ],
   "source": [
    "batch = dataset_test_sft['text'][2:3]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    batch, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=current_max_len_qlora\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=min(512, MAX_SEQ_LENGTH),\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        pad_token_id=tokenizer_sft.pad_token_id,\n",
    "        eos_token_id=tokenizer_sft.eos_token_id  # Often helpful to add\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer_sft.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9545d2c1-fdf4-4e33-9434-a7ab587e1fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1274/2496646625.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π <span style=\"color: red; text-decoration: line-through;\">–¥–∏—Ä–µ—Ç–æ—Ä</span> <span style=\"color: green;\">–¥–∏—Ä–µ–∫—Ç–æ—Ä</span> —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª <span style=\"color: red; text-decoration: line-through;\">–ê–ª–µ–∫—Å–∞–Ω–¥–µ—Ä</span> <span style=\"color: green;\">–ê–ª–µ–∫—Å–∞–Ω–¥—Ä</span> –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª <span style=\"color: red; text-decoration: line-through;\">(–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ</span> <span style=\"color: green;\">(–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ,</span> –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ <span style=\"color: red; text-decoration: line-through;\">–ø–æ–º–æ—à–Ω–∏–∫–æ–≤</span> <span style=\"color: green;\">–ø–æ–º–æ—â–Ω–∏–∫–æ–≤</span> –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª <span style=\"color: red; text-decoration: line-through;\">—Ä–µ—à–µ–Ω–∏–µ,</span> <span style=\"color: green;\">—Ä–µ—à–µ–Ω–∏–µ</span> –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ <span style=\"color: red; text-decoration: line-through;\">–º–∞—Ç—á,</span> <span style=\"color: green;\">–º–∞—Ç—á</span> –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ <span style=\"color: red; text-decoration: line-through;\">–∏–≥—Ä—ã,</span> <span style=\"color: green;\">–∏–≥—Ä—ã</span> –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ <span style=\"color: red; text-decoration: line-through;\">–∞—Ä–±–∏—Ç—Ä,</span> <span style=\"color: green;\">–∞—Ä–±–∏—Ç—Ä</span> –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä <span style=\"color: red; text-decoration: line-through;\">–ö—É—Ü–µ–≤,</span> <span style=\"color: green;\">–ö—É—Ü–µ–≤</span> –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç <span style=\"color: red; text-decoration: line-through;\">—Å–µ—Ä—å—ë–∑–Ω—ã–µ</span> <span style=\"color: green;\">—Å–µ—Ä—å–µ–∑–Ω—ã–µ</span> —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ <span style=\"color: red; text-decoration: line-through;\">–ì–∞–π–¥–∞—à–∞,</span> <span style=\"color: green;\">–ì–∞–π–¥–∞—à–∞</span> –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± <span style=\"color: red; text-decoration: line-through;\">–æ—Ç–º–µ–Ω–µ,</span> <span style=\"color: green;\">–æ—Ç–º–µ–Ω–µ</span> –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª <span style=\"color: red; text-decoration: line-through;\">(2:1),</span> <span style=\"color: green;\">(2:1)</span> –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ <span style=\"color: red; text-decoration: line-through;\">–∫–æ—Ä–∏–¥–æ—Ä–µ</span> <span style=\"color: green;\">–∫–æ—Ä–∏–¥–æ—Ä–µ,</span> –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ <span style=\"color: red; text-decoration: line-through;\">–ì–∞–¥–∂–∏–µ–≤–∞</span> <span style=\"color: green;\">–ì–∞–¥–∂–∏–µ–≤–∞,</span> –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ. <span style=\"color: green;\">#–¢—É—Ä–Ω–∏—Ä</span> <span style=\"color: green;\">#–§—É—Ç–±–æ–ª</span> <span style=\"color: green;\">#–°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å</span> <span style=\"color: green;\">#–ì–≤–∞—Ä–¥–µ–π—Ü—ã</span> <span style=\"color: green;\">#–ì–∞–π–¥–∞</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from difflib import ndiff\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def format_diffs(original, corrected):\n",
    "    diffs = list(ndiff(original.split(), corrected.split()))\n",
    "    formatted_text = \"\"\n",
    "    for diff in diffs:\n",
    "        if diff.startswith('-'):\n",
    "            formatted_text += f'<span style=\"color: red; text-decoration: line-through;\">{diff[2:]}</span> '\n",
    "        elif diff.startswith('+'):\n",
    "            formatted_text += f'<span style=\"color: green;\">{diff[2:]}</span> '\n",
    "        elif diff.startswith(' '):\n",
    "            formatted_text += diff[2:] + ' '\n",
    "    return formatted_text\n",
    "\n",
    "input_text = '–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥–µ—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—à–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ, –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á, –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã, –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä, –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤, –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å—ë–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞, –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ, –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1), –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞ –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ.'\n",
    "output_text = '–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä —Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–æ–π ¬´–¢–∞–≤—Ä–∏–∏¬ª –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–∞–π–¥–∞—à –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ –ø—Ä–æ—Ç–∏–≤ ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª (–ì–≤–∞—Ä–¥–µ–π—Å–∫–æ–µ, –°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å—Å–∫–∏–π —Ä–∞–π–æ–Ω) –Ω–∞–ø–∞–ª –Ω–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –≥–ª–∞–≤–Ω–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏ –†—É—Å–ª–∞–Ω–∞ –°–µ–ª–∏–º–æ–≤–∞. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç Matcday. –ö–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–≥—Ä–µ —Ç—Ä–µ—Ç—å–µ–≥–æ —Ç—É—Ä–∞ –í—Å–µ–∫—Ä—ã–º—Å–∫–æ–≥–æ —Ç—É—Ä–Ω–∏—Ä–∞-2015. –ì–∞–π–¥–∞—à –≤–æ—Ä–≤–∞–ª—Å—è –≤ —Å—É–¥–µ–π—Å–∫—É—é –∏ –Ω–∞–Ω–µ—Å –°–µ–ª–∏–º–æ–≤—É –Ω–µ—Å–∫–æ–ª—å–∫–æ —É–¥–∞—Ä–æ–≤. –ü–æ—á—Ç–∏ —Å—Ä–∞–∑—É –±—ã–ª –≤—ã–∑–≤–∞–Ω –Ω–∞—Ä—è–¥ –ø–æ–ª–∏—Ü–∏–∏, –Ω–æ –≥–ª–∞–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä –≤—Å—Ç—Ä–µ—á–∏ –î–º–∏—Ç—Ä–∏–π –ì—Ä–∞—á–µ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–µ–¥–∏–Ω–æ–∫. –í—ã—à–µ–ª –Ω–∞ –º–∞—Ç—á –∏ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Ä–µ—Ñ–µ—Ä–∏. –ü–æ—Å–ª–µ –∏–≥—Ä—ã –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Ç–æ–∫–æ–ª. –°–≤–∏–¥–µ—Ç–µ–ª—è–º–∏ –≤—ã—Å—Ç—É–ø–∏–ª–∏ –∞—Ä–±–∏—Ç—Ä –∏ –≤—Ç–æ—Ä–æ–π –ø–æ–º–æ—â–Ω–∏–∫. –ò–Ω—Å–ø–µ–∫—Ç–æ—Ä –º–∞—Ç—á–∞ –í–ª–∞–¥–∏–º–∏—Ä –ö—É—Ü–µ–≤ –æ—Ç–º–µ—Ç–∏–ª —ç–ø–∏–∑–æ–¥ –≤ —Ä–∞–ø–æ—Ä—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª –µ–≥–æ –≤ –§–µ–¥–µ—Ä–∞—Ü–∏—é —Ñ—É—Ç–±–æ–ª–∞ –ö—Ä—ã–º–∞. –¢–µ–ø–µ—Ä—å –ì–∞–π–¥–∞—à—É –≥—Ä–æ–∑—è—Ç —Å–µ—Ä—å–µ–∑–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ì–∞–π–¥–∞—à–∞ –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ–º–æ—â–Ω–∏–∫–∞ —Ä–µ—Ñ–µ—Ä–∏ –±—ã–ª–∞ —Å–ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞–Ω–∞ —Ä–µ—à–µ–Ω–∏–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –æ–± –æ—Ç–º–µ–Ω–µ –≤ –ø–µ—Ä–≤–æ–º —Ç–∞–π–º–µ –≥–æ–ª–∞ ¬´–¢–∞–≤—Ä–∏–∏¬ª –∑–∞ –Ω–∞–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –≤—Ä–∞—Ç–∞—Ä—è —Å–æ–ø–µ—Ä–Ω–∏–∫–æ–≤. –í—Å—Ç—Ä–µ—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –ø–æ–±–µ–¥–æ–π ¬´–ì–≤–∞—Ä–¥–µ–π—Ü–∞¬ª —Å–æ —Å—á–µ—Ç–æ–º 1:0. –í –∞–≤–≥—É—Å—Ç–µ 2011 –≥–æ–¥–∞ –≤ –ø–µ—Ä–µ—Ä—ã–≤–µ –º–∞—Ç—á–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ –†–æ—Å—Å–∏–∏ –ø–æ —Ñ—É—Ç–±–æ–ª—É –º–µ–∂–¥—É –º–∞—Ö–∞—á–∫–∞–ª–∏–Ω—Å–∫–∏–º ¬´–ê–Ω–∂–∏¬ª –∏ –º–æ—Å–∫–æ–≤—Å–∫–∏–º ¬´–î–∏–Ω–∞–º–æ¬ª (2:1) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–ª–æ–º–∞–ª –¥–≤–µ—Ä—å –≤ –∫–æ—Ä–∏–¥–æ—Ä–µ, –≤–µ–¥—É—â–µ–º –≤ —Å—É–¥–µ–π—Å–∫—É—é –∫–æ–º–Ω–∞—Ç—É –∏ –∫–æ–º–Ω–∞—Ç—É –¥–µ–ª–µ–≥–∞—Ç–∞ –º–∞—Ç—á–∞. –ü–æ–¥–æ–∑—Ä–µ–Ω–∏—è –ø–∞–ª–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã –∏–∑ –î–∞–≥–µ—Å—Ç–∞–Ω–∞ –ì–∞–¥–∂–∏ –ì–∞–¥–∂–∏–µ–≤–∞, –Ω–æ –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–∞–∫ –∏ –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ.  #–¢—É—Ä–Ω–∏—Ä #–§—É—Ç–±–æ–ª #–°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å #–ì–≤–∞—Ä–¥–µ–π—Ü—ã #–ì–∞–π–¥–∞'\n",
    "\n",
    "display(HTML(format_diffs(input_text, output_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040ade6-fcf2-4657-af4f-3d1c7f1f8699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
